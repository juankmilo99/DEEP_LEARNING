# -*- coding: utf-8 -*-
"""Analisis de Sentimientos TWITTER.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12kr7Hwiw1UQvhK-tRxxq-zYww5lAHJQV

#Conexion GoogleDrive, archivos csv
"""

from google.colab import drive
drive.mount('/content/gdrive')

import os
import tweepy as tw
import pandas as pd

consumer_key= 'h4ncdpREIlV0WU6MyMt2M8Chy'
consumer_secret= 'CbT6rW05EZK59y3JCo4vfmeuqxw8YGIiCIrnU1mI1WMrb6yhU4'
access_token= '1099177832540979201-sR0OnajEXYeoXHX9LstbjPcm8V2LgI'
access_token_secret= 'jvZN4orgHq13oQOcfTz1FhsS8UNwGlvBcRWlLUXUYQTtG'

auth = tw.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
api = tw.API(auth, wait_on_rate_limit=True)

# Definir el termino de la busqueda y la fecha de inicio
search_words = '#AgroIngresoSeguro2'
date_since = '2020-01-01'
#Para que no tome los retweets
new_search = search_words + " -filter:retweets"
new_search

# Collecional tweets
tweets = tw.Cursor(api.search,
              q=new_search,
              lang='es',
              since=date_since).items(1000)

tw_dataframe = pd.DataFrame(data= dtweets,columns=["user","location","text"])
tw_dataframe[:10]

tw_dataframe.to_csv("/content/drive/My Drive/Analisis de Sentimientos/datasets/DATASET_AgroIngresoSeguro2_JAGP.csv", index=False, encoding='utf-8')

dataframe = tw_dataframe
dataframe.head(10)

"""##P0. Obtener corpus : Convirtiendo XML a CSV"""

#librerias necesarias
import xml.etree.ElementTree as etree
import csv
from os import scandir
from sklearn.model_selection import train_test_split

"""###Funcion para listar archivos de un directorio"""

# importing os module  
import os 
#listado_de_archivos_desde_un_path
def files_of_path(path): 
    return [obj.name for obj in os.scandir(path) if obj.is_file()]
    
files= files_of_path("/content/gdrive/My Drive/Analisis de Sentimientos/datasets/tass2014/")
for file in files:
    print(file)

"""###función para convertir listas en archivos CSV"""

def list_to_csv(data, filename):
  with open(filename, 'w', encoding='utf-8') as csvfile:
    writer = csv.writer(csvfile, delimiter=',', lineterminator='\n', quoting=csv.QUOTE_NONNUMERIC)
    writer.writerows(data)

"""###función para cargar de un CSV a  una LISTA (messages | labels)"""

def csv_to_lists(filename):
  messages = []
  labels = []
  with open(filename, 'r', encoding='utf-8') as csvfile:
    reader = csv.reader(csvfile, delimiter=',')
    for row in reader:
      messages.append(row[1])
      labels.append(row[2])
  return messages, labels

"""###funciones para prasear xml en un dataframe

####corpus de general  tweetid | content | sentiments/polarity/value
"""

def general_tass_to_list(filename):
  tree = etree.parse(filename)
  root = tree.getroot()
  data = []

  for tweet in root:
    tweetId = tweet.find('tweetid').text
    content = tweet.find('content').text
    polarityValue = tweet.find('sentiments/polarity/value').text
    data.append([tweetId, content.replace('\n',' '), polarityValue])
  return data

def general_tass_2017_to_list(filename,qrel=None):
  tree = etree.parse(filename)
  root = tree.getroot()
  data = []

  for tweet in root:
    tweetId = tweet.find('tweetid').text
    content = tweet.find('content').text
    polarityValue = qrel[tweetId]
    data.append([tweetId, content.replace('\n',' '), polarityValue])

  return data

"""####Corpus politics  tweetid | content | sentiments/polarity"""

def politics_tass_to_list(filename):
  tree = etree.parse(filename)
  root = tree.getroot()
  data = []

  for tweet in root:
    tweetId = tweet.find('tweetid').text
    content = tweet.find('content').text
    aux = next((e for e in tweet.findall('sentiments/polarity') if e.find('entity') == None), None)
    if aux != None:
      polarityValue = aux.find('value').text
      data.append([tweetId, content.replace('\n',' '), polarityValue])
  return data

"""####corpus de internacional  tweetid | content | sentiments/polarity/value"""

def intertass_tass_to_list(filename, qrel=None):
  tree = etree.parse(filename)
  root = tree.getroot()
  data = []

  for tweet in root:
    tweetId = tweet.find('tweetid').text
    content = tweet.find('content').text
    polarityValue = tweet.find('sentiment/polarity/value').text
    if polarityValue == None:
      polarityValue = qrel[tweetId]
      data.append([tweetId, content.replace('\n',' '), polarityValue])
  return data

"""####Parcear corpus de social-TV (no esta terminado)"""

from lxml import etree
doc = etree.parse('/content/gdrive/My Drive/Analisisde Sentimientos/datasets/tass2014/Social-TV/socialtv-tweets-test.xml')
#print (etree.tostring(doc,pretty_print=True ,xml_declaration=True, encoding="utf-8"))

raiz=doc.getroot()
#print (raiz.tag)
#print (len(raiz))
tweet=raiz[1]
print (tweet.text)
print (tweet.attrib)
print (tweet[0].text)
for attr,value in tweet.items():
  print (attr,value)
print (tweet.get("sentiment"))

from xml.dom import minidom
def social_tv_to_list(filename):
  xmldoc = minidom.parse(filename)
  tweetlist = xmldoc.getElementsByTagName('tweet')
  print(len(tweetlist))
  print(tweetlist[0].attributes['id'].value)
  sentimentlist = xmldoc.getElementsByTagName('sentiment')
  print(sentimentlist[0].attributes['aspect'].value)
  #for s in tweetlist:
#    print(s.attributes['id'].value)

social_tv_to_list("/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/tass2014/Social-TV/socialtv-tweets-test.xml")

"""####funcion para unir los tweets corpus general test con sus sentimientos"""

#Listar los id tweets | sentiment :P (Positivo) - N (Negativo) - NEU (NEUtro) - NONE (sin sentimiento)
def gold_standard_to_dict(filename):
  with open(filename, 'r') as csvfile:
    reader = csv.reader(csvfile, delimiter='\t')
    data = {rows[0]: rows[1] for rows in reader}

  return data

"""###Función para separar el 100% del corpus entre: Train : 70% - Test: 30%"""

def generate_train_test_subsets(data, size):
  codes = [d[0] for d in data]
  labels = [d[2] for d in data]
  codes_train, codes_test, labels_train, labels_test = train_test_split(codes, labels, train_size=size)
  train_data = [d for d in data if d[0] in codes_train]
  test_data = [d for d in data if d[0] in codes_test]
  return train_data, test_data

"""### **Ejecutar cada función de parsear los corpus y guardarlo en un CSV (full, train, test)**"""

data = []

#Parceamos el internacional TASS
#tomamos el corpus internacional (test) y generamos una lista del ID del tweet y el sentimiento para agregarlo a la data
qrel = gold_standard_to_dict("/content/gdrive/My Drive/Analisis de Sentimientos/datasets/tass2014/intertass-sentiment.qrel")
#como el test del corpus internacional esta sin los sentimientos es necesario agregarlos : qrel
data.extend(intertass_tass_to_list("/content/gdrive/My Drive/Analisis de Sentimientos/datasets/tass2014/intertass-test.xml", qrel))
data.extend(intertass_tass_to_list("/content/gdrive/My Drive/Analisis de Sentimientos/datasets/tass2014/intertass-development-tagged.xml"))
data.extend(intertass_tass_to_list("/content/gdrive/My Drive/Analisis de Sentimientos/datasets/tass2014/intertass-train-tagged.xml"))
#Parceamos el General
data.extend(general_tass_to_list("/content/gdrive/My Drive/Analisis de Sentimientos/datasets/tass2014/general-test-tagged-3l.xml"))
data.extend(general_tass_to_list("/content/gdrive/My Drive/Analisis de Sentimientos/datasets/tass2014/general-train-tagged-3l.xml"))
#qrel = gold_standard_to_dict("/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/tass_2017/General Corpus of TASS/general-sentiment-3l.qrel")
#data.extend(general_tass_2017_to_list("/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/tass_2017/General Corpus of TASS/general-tweets-test.xml", qrel))
#Parceamos el STOMPOL (politica)
data.extend(politics_tass_to_list("/content/gdrive/My Drive/Analisis de Sentimientos/datasets/tass2014/politics-test-tagged.xml"))
#separamos la data en train = 70%  | test = 30#
#test, train  = generate_train_test_subsets(data, size=0.3)
list_to_csv(data,"/content/gdrive/My Drive/Analisis de Sentimientos/datasets/dataset_2014_full.csv")
#list_to_csv(train, '/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/dataset_2017_train.csv')
#list_to_csv(test, '/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/dataset_2017_test.csv')

"""##P1. Preprocesamiento del corpus

###Cargar librerías necesarias
"""

import re                                #operaciones regulares para la búsqueda y manipulación de cadenas
from nltk import TweetTokenizer          #libreria para tokenizar
from nltk.stem import SnowballStemmer    #algoritmo para clasificación de palabras
#variables para mejorar la escritura (opcional)
NORMALIZE = 'normalize'
REMOVE = 'remove'
MENTION = 'twmention'
HASHTAG = 'twhashtag'
URL = 'twurl'
LAUGH = 'twlaugh'

#definir que el algoritmo de clasificación use el idioma español
_stemmer = SnowballStemmer('spanish')

#definir una variable para la funcion de tokenizar (opcional)
_tokenizer = TweetTokenizer().tokenize

#variable para definir si quiero normalizar: normalize o eliminar: remove los hashtags, menciones y urls en los tweets
_twitter_features="normalize"
#variable para definir si se desea tener convertir o no a la raiz de la palabra.
_stemming=False

"""### funciones/métodos de preprocesamiento

####listas de conversión (quitar tildes y palabras coloquiales)
"""

#lista de conversión para quitar las tildes a las vocales.
DIACRITICAL_VOWELS = [('á','a'), ('é','e'), ('í','i'), ('ó','o'), ('ú','u'), ('ü','u')]

#lista para corregir algunas palabras coloquiales / jerga en español (obviamente faltan más)
SLANG = [('d','de'), ('[qk]','que'), ('xo','pero'), ('xa', 'para'), ('[xp]q','porque'),('es[qk]', 'es que'),
         ('fvr','favor'),('(xfa|xf|pf|plis|pls|porfa)', 'por favor'), ('dnd','donde'), ('tb', 'también'),
         ('(tq|tk)', 'te quiero'), ('(tqm|tkm)', 'te quiero mucho'), ('x','por'), ('\+','mas')]

"""#### funcion/método de normalización de risas"""

#metodo para normalizar las risas
def normalize_laughs(message):
  message = re.sub(r'\b(?=\w*[j])[aeiouj]{4,}\b', LAUGH, message, flags=re.IGNORECASE)
  message = re.sub(r'\b(?=\w*[k])[aeiouk]{4,}\b', LAUGH, message, flags=re.IGNORECASE)
  message = re.sub(r'\b(juas+|lol)\b', LAUGH, message, flags=re.IGNORECASE)
  return message

print (normalize_laughs("esto muyy feliz jajajajaja o no tan feliz jejejejeje o mejor me rio a como papa noel JOJOJO o como en mileniams LOL  kakaka"))

"""####Función/método para eliminar o normalizar  menciones, hashtags y URL de un mensaje (tweet)"""

def process_twitter_features(message, twitter_features):

  message = re.sub(r'[\.\,]http','. http', message, flags=re.IGNORECASE)
  message = re.sub(r'[\.\,]#', '. #', message)
  message = re.sub(r'[\.\,]@', '. @', message)

  if twitter_features == REMOVE:
    # eliminar menciones, hashtags y URL
    message = re.sub(r'((?<=\s)|(?<=\A))(@|#)\S+', '', message)
    message = re.sub(r'\b(https?:\S+)\b', '', message, flags=re.IGNORECASE)
  elif twitter_features == NORMALIZE:
    # cuando sea necesario se normalizaran las menciones, hashtags y URL
    message = re.sub(r'((?<=\s)|(?<=\A))@\S+', MENTION, message)
    message = re.sub(r'((?<=\s)|(?<=\A))#\S+', HASHTAG, message)
    message = re.sub(r'\b(https?:\S+)\b', URL, message, flags=re.IGNORECASE)

  return message

print(process_twitter_features("Rosell, una noche. Adivina quien!! http://t.co/PPAwijRX","normalize"))

"""####Función/método general para el preprocesamiento"""

def preprocess(message):
  # convertir a minusculas
  message = message.lower()
        
  # eliminar números, retorno de linea y el tan odios retweet (de los viejos estilos de twitter)
  message = re.sub(r'(\d+|\n|\brt\b)', '', message)
        
  # elimar vocales con signos diacríticos (posible ambigüedad)
  for s,t in DIACRITICAL_VOWELS:
    message = re.sub(r'{0}'.format(s), t, message)
        
  # eliminar caracteres repetidos 
  message = re.sub(r'(.)\1{2,}', r'\1\1', message)
       
  # normalizar las risas
  message = normalize_laughs(message)
        
  # traducir la jerga y terminos coloquiales sobre todo en el español
  for s,t in SLANG:
    message = re.sub(r'\b{0}\b'.format(s), t, message)

  #normalizar/eliminar hashtags, menciones y URL
  message = process_twitter_features(message, _twitter_features)

  #Convertir las palabras a su raiz ( Bonita, bonito) -> bonit 
  if _stemming:
    message = ' '.join(_stemmer.stem(w) for w in _tokenizer(message))

  return message

print(preprocess("LOL!! muy graciosa está PAGUINA  https://actualidadpanamericana.com :-) jajajaja muy buena"))

"""###Descargamos la librerias  NLTK -"""

#Descargamos la libreria de stopwords
import nltk
nltk.download('stopwords')

"""### Cargamos el CSV del corpus de google drive

## CONEXION DRIVE!!
"""

from google.colab import drive
drive.mount('/content/drive')

"""###Aplicamos preprocesamiento al CSV y creamos un nuevo CSV limpio"""

import numpy as np
import pandas as pd

df = pd.read_csv('/content/drive/My Drive/Analisis de Sentimientos/datasets/dataset_2014_full.csv', encoding='utf-8')
#asignamos nombres a las columnas del csv para facilitar la busqueda de información
df.columns = ['tweetid', 'tweet','sentiment']
#aplicamos el preprocesamiento a los tweets con steaming =false
df['tweet'] = df['tweet'].apply(preprocess)
#eliminamos la columna tweetid que no nos sirve para entrenar y si nos genera mas uso de memoria 
df = df.drop(columns="tweetid")
#Es mejor trabajar con valores enteros que con letras
#por lo tanto reemplazaremos los sentimientos que estan como NONE->-1 | NEU -> 0 | P->1 | N->2
df.loc[df['sentiment'] == 'NONE', 'sentiment'] = '-1'
df.loc[df['sentiment'] == 'NEU', 'sentiment'] = '0'
df.loc[df['sentiment'] == 'P', 'sentiment'] = '1'
df.loc[df['sentiment'] == 'N', 'sentiment'] = '2'
df["sentiment"].unique()
#guardamos el dataset en un nuvevo CSV para facilitar su posterior uso
df.to_csv('/content/drive/My Drive/Analisis de Sentimientos/datasets/dataset_2014_full_Clean.csv', index=False, encoding='utf-8')

"""##P2.Entrenando el modelo de aprendizaje

###Funciones de tokenizar/extraer tweets
"""

#p2.1: funcion tokenizar con esteroides --tokeniza y limpia--
print("p2.1: funcion tokenizar con esteroides --tokeniza y limpia--")
def tokenizer(text):
    text = re.sub('<[^>]*>', '', text)
    emoticons = re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)', text.lower())
    text = re.sub('[\W]+', ' ', text.lower()) +' '.join(emoticons).replace('-', '')
    tokenized = [w for w in text.split() if w not in stop]
    return tokenized
#p2.2: funcion para extraer un documento del dataset  
print("p2.2: funcion para extraer un documento del dataset  ")
def stream_docs(path):
    with open(path, 'r', encoding='utf-8') as csv:
        next(csv)  # skip header
        for line in csv:
            text, label = line[:-3],  int(line[-2])
            yield text, label
#p2.3: funcion que tomara una secuencia de documentos y devolvera un número particular de documentos
def get_minibatch(doc_stream, size):
    docs, y = [], []
    try:
        for _ in range(size):
            text, label = next(doc_stream)
            docs.append(text)
            y.append(label)
    except StopIteration:
        return None, None
    return docs, y

next(stream_docs(path='/content/drive/My Drive/Analisis de Sentimientos/datasets/dataset_2014_full_Clean.csv'))

"""###Entrenamos el modelo usando regresión logistica
usaremos regresión logistica xq es menos costoso en tiempo de procesamiento que support vector machine
"""

pip install pyprind

path='/content/drive/My Drive/Analisis de Sentimientos/datasets/dataset_2014_full_Clean.csv'
#p2: definimos una versión liviana de CountVectorizer+TfidfVectorizer llamada HashingVectorizer
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.linear_model import SGDClassifier

vect = HashingVectorizer(decode_error='ignore', 
                         n_features=2**21,
                         preprocessor=None, 
                         tokenizer=tokenizer)

#definimos como algoritmo la regressión logistica en el decenso gradiante 

clf = SGDClassifier(loss='log', random_state=1, max_iter=1)
doc_stream = stream_docs(path)
#p3. entrenamos 
import re
import numpy as np
import pyprind
from nltk.corpus import stopwords
stop = stopwords.words('spanish')
pbar = pyprind.ProgBar(50)
#definimos las clases con las cuales vamos a entrenar
classes = np.array([-1,0, 1,2])
#hacemos 50 repeticiones
for _ in range(50):
  #tomaremos grupos de 500 tweets para entrenar
    X_train, y_train = get_minibatch(doc_stream, size=500)
    if not X_train:
        break
    X_train = vect.transform(X_train)
    clf.partial_fit(X_train, y_train, classes=classes)
    pbar.update()
#probamos la eficiencia del modelo con 500 tweets .
X_test, y_test = get_minibatch(doc_stream, size=500)
X_test = vect.transform(X_test)
print('Presición del modelo: %.3f' % clf.score(X_test, y_test))
#recalibramos el modelo.
clf = clf.partial_fit(X_test, y_test)

"""##P3.Serializamos (congelamos) el modelo para usarlo fuera de google colaboratory"""

import pickle
import os
#creo una carpeta en mi google drive para guardar los archivos serializados
dest = os.path.join('/content/drive/My Drive/Analisis de Sentimientos/twitterclassifier', 'pkl_objects')
if not os.path.exists(dest):
    os.makedirs(dest)
#convertimos el clasificador y el stopword en archivo/objectos pkl
pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=4)   
pickle.dump(clf, open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol=4)
#Es importante recordar que deben verificar que los dos archivos esten en su drive

"""###Probemos a ver si funciona
Cambiamos la basepath (directorio por defecto) de Python a la carpeta de **Twitterclassifier**
"""

import os
os.chdir('/content/drive/My Drive/Analisis de Sentimientos/twitterclassifier')

"""####Deserializamos los estimadores"""

import pickle
import re
import os
from vectorizer import vect
clf = pickle.load(open(os.path.join('pkl_objects', 'classifier.pkl'), 'rb'))

"""#### Clasifiquemos un texto"""

import numpy as np
#NONE->-1 | NEU -> 0 | P->1 | N->2
label = {-1:'Sin sentimiento', 0:'Neutro', 1:'Positivo',2: 'Negativo'}
example1 = 'Duque es el peor presidente'
example = [example1]
#convertimos el texto en un vector de palabras y extraemos sus caracteristicas https://scikit-learn.org/stable/modules/feature_extraction.html
textConvert = vect.transform(example)
print('*Predicción: %s\n*Probabilidad: %.2f%%'%(label[clf.predict(textConvert)[0]], np.max(clf.predict_proba(textConvert))*100))

"""###**RECORREMOS LOS TWEETS DESCARGADOS Y LOS CLASIFICAMOS**"""

import numpy as np
import pandas as pd
import pyprind

df = pd.read_csv('/content/drive/My Drive/Analisis de Sentimientos/datasets/DATASET_NoEstamosSolos_JAGP.csv', encoding='utf-8')
#creamos una columna llamada Sentimient donde guardaremos la predicción
df['sentiment'] = ''
#creamos una columna llamada Probability donde guardaremos la acertabilidad que dio el clasificador
df['probability']= 0
#conversión de sentimientos (numeros a palabras)= NONE->-1 | NEU -> 0 | P->1 | N->2
label = {-1:'Sin sentimiento', 0:'Neutro', 1:'Positivo',2: 'Negativo'}
for rowid in range(len(df.index)):
  text=df['text'][rowid]
  textConvert = vect.transform([text]) 
  df['sentiment'][rowid]=label[clf.predict(textConvert)[0]]
  df['probability'][rowid]=np.max(clf.predict_proba(textConvert))*100
  pbar.update()
#df.head(20)
df.to_csv('/content/drive/My Drive/Analisis de Sentimientos/datasets/DATASET_NoEstamosSolos_JAGP.csv', index=False, encoding='utf-8')

#-----opcional----
#segunda forma de ejecutar el analisis (metodos)
def f_prediction(row):
  text=row['text']
  textConvert = vect.transform([text]) 
  return label[clf.predict(textConvert)[0]]

def f_probability(row):
  text=row['text']
  textConvert = vect.transform([text]) 
  return np.max(clf.predict_proba(textConvert))*100

df["sentiment"] = df.apply(f_prediction, axis=1) # recorriendo columnas
df["probability"] = df.apply(f_probability, axis=1) # recorriendo columnas

import matplotlib.pyplot as plt
#sentimientos = df["sentiment"].unique()
df.groupby('sentiment')['location'].nunique().plot(kind='bar')
print(df.groupby(['sentiment']).size())
#df.groupby(['sentiment']).size().unstack().plot(kind='bar',stacked=True)
plt.show()

#df.head(20)
#df["sentiment"].unique()
