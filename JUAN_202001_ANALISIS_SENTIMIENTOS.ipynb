{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JUAN 202001_ANALISIS_SENTIMIENTOS.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_nOfJ_1K0si",
        "colab_type": "text"
      },
      "source": [
        "#**ANALISIS DE SENTIMIENTOS EN INGLES USANDO ACLIMDB**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "968LBC6YoA4A",
        "colab_type": "text"
      },
      "source": [
        "##Obteniendo corpus de aclimdb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP2jLRomNq8H",
        "colab_type": "code",
        "outputId": "f173eed4-3850-4e27-ab04-2290ef623fc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# instalar librería para visualizar el progreso de ejecución una tarea en background\n",
        "pip install pyprind  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyprind\n",
            "  Downloading https://files.pythonhosted.org/packages/1e/30/e76fb0c45da8aef49ea8d2a90d4e7a6877b45894c25f12fb961f009a891e/PyPrind-2.11.2-py3-none-any.whl\n",
            "Installing collected packages: pyprind\n",
            "Successfully installed pyprind-2.11.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YPjnDkRKq2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os            #  trabajar sobre el sistema operativo\n",
        "import sys           #  manipular archivos (cortar, copiar, borrar, crear)\n",
        "import tarfile       #  Manipular archivos comprimidos (comprimir, descomprimir)\n",
        "import time          #  calcular tiempo (en este caso tiempo de descarga de archivo)\n",
        "import pyprind\n",
        "import pandas as pd\n",
        "\n",
        "basepath = 'aclImdb'\n",
        "source = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "target = 'aclImdb_v1.tar.gz'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABUOzdraoEOT",
        "colab_type": "text"
      },
      "source": [
        "###función para ver avance de proceso en *background*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EoFo2aPMMO5",
        "colab_type": "code",
        "outputId": "77a474b2-6684-4a36-e730-f88a3468e22f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "def reporthook(count, block_size, total_size):\n",
        "    global start_time\n",
        "    if count == 0:\n",
        "        start_time = time.time()\n",
        "        return\n",
        "    duration = time.time() - start_time\n",
        "    progress_size = int(count * block_size)\n",
        "    speed = progress_size / (1024.**2 * duration)\n",
        "    percent = count * block_size * 100. / total_size\n",
        "    sys.stdout.write(\"\\r%d%% | %d MB | %.2f MB/s | %d segundos transcurrido\" %\n",
        "                    (percent, progress_size / (1024.**2), speed, duration))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "if not os.path.isdir('aclImdb') and not os.path.isfile('aclImdb_v1.tar.gz'):\n",
        "    \n",
        "    if (sys.version_info < (3, 0)):\n",
        "        import urllib\n",
        "        urllib.urlretrieve(source, target, reporthook)\n",
        "    \n",
        "    else:\n",
        "        import urllib.request\n",
        "        urllib.request.urlretrieve(source, target, reporthook)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-15e747fd580d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'aclImdb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'aclImdb_v1.tar.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMNt5_wWLF1Z",
        "colab_type": "text"
      },
      "source": [
        "### Extraemos todos los archivos del corpus "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiRUWiaANHaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.isdir('aclImdb'):\n",
        "\n",
        "    with tarfile.open(target, 'r:gz') as tar:\n",
        "        tar.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SokQ64p5nvic",
        "colab_type": "text"
      },
      "source": [
        "###Funcion para convertir archivos txt en un dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG-anmqPOlnW",
        "colab_type": "code",
        "outputId": "25e7929f-8f3d-466f-ee63-ca8b229fccb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "labels = {'pos': 1, 'neg': 0}\n",
        "pbar = pyprind.ProgBar(50000)\n",
        "\n",
        "df = pd.DataFrame()\n",
        "\n",
        "for s in ('test', 'train'):\n",
        "    for l in ('pos', 'neg'):\n",
        "        path = os.path.join(basepath, s, l)\n",
        "        for file in os.listdir(path):\n",
        "            with open(os.path.join(path, file), \n",
        "                      'r', encoding='utf-8') as infile:\n",
        "                txt = infile.read()\n",
        "            df = df.append([[txt, labels[l]]], \n",
        "                           ignore_index=True)\n",
        "            pbar.update()\n",
        "\n",
        "df.columns = ['review', 'sentiment']\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0% [##############################] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:01:21\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ9lDR0npHou",
        "colab_type": "text"
      },
      "source": [
        "###Guardando Dataframe en CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO_r_-BdPx8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)\n",
        "df = df.reindex(np.random.permutation(df.index))\n",
        "df.to_csv('movie_data.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ldGm4X5MfTj",
        "colab_type": "text"
      },
      "source": [
        "##Preprocesamiento del corpus "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68XefC8xQMR1",
        "colab_type": "text"
      },
      "source": [
        "###Conectarmen a google drive y cargar el CSV de criticas de cine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7xtGQqQqTCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3aqFb73QxPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame()\n",
        "df = pd.read_csv('/content/gdrive/My Drive/USTA-202001/USTA-202001_7°_DEEP_LEARNING/Datasets/movie_data.csv', encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUMScms2TODd",
        "colab_type": "text"
      },
      "source": [
        "###Vectorizando texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UuLmL2jTPzl",
        "colab_type": "code",
        "outputId": "2f2a1db1-dba5-4841-838b-adfdb5ebf649",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "new_text = ['probando un texto para pruebas de texto']\n",
        "vector= CountVectorizer(stop_words=None)\n",
        "vector.fit(new_text)\n",
        "\n",
        "print(vector.vocabulary_)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'probando': 2, 'un': 5, 'texto': 4, 'para': 1, 'pruebas': 3, 'de': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdXt41hUU08l",
        "colab_type": "code",
        "outputId": "5e50234d-677b-4807-a5b1-6d9b19b5e946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count = CountVectorizer()\n",
        "docs = np.array([\n",
        "        'The sun is shining',\n",
        "        'The weather is sweet',\n",
        "        'The sun is shining, the weather is sweet, and one and one is two'])\n",
        "bag = count.fit_transform(docs)\n",
        "\n",
        "print(count.vocabulary_)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2Aq4cLPVbQ4",
        "colab_type": "code",
        "outputId": "2b14a439-7c94-4654-bc88-0dc86ad4957c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(bag.toarray())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 0 1 1 0 1 0 0]\n",
            " [0 1 0 0 0 1 1 0 1]\n",
            " [2 3 2 1 1 1 2 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z50v53mtMwj-",
        "colab_type": "text"
      },
      "source": [
        "###Extracción de características "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54aCGRWaPPq6",
        "colab_type": "text"
      },
      "source": [
        "####TfidfTransformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDhMXl6yPY11",
        "colab_type": "code",
        "outputId": "0866baff-b8cf-4f0c-9c8a-03c896824dc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf = TfidfTransformer(use_idf=True, \n",
        "                         norm='l2', \n",
        "                         smooth_idf=True)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]\n",
            " [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]\n",
            " [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvquv5h2REK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head(7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BgLZO4TRb3z",
        "colab_type": "text"
      },
      "source": [
        "####funcion para limpiar textos de emoticones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VS4XmdrnRgpN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "# creamos una funcion llamada preprocessor\n",
        "def preprocessor(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\n",
        "    text = (re.sub('[\\W]+', ' ', text.lower()) +' '.join(emoticons).replace('-', ''))\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETxFZSMwUc5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['review'] = df['review'].apply(preprocessor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBCYt1WiVZO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "#nltk.download(“all\")\n",
        "#nltk.download(\"popular\")\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsfmDqa3WQoH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "def tokenizer_porter(text):\n",
        "    return [porter.stem(word) for word in text.split()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQcWrE3tWCMA",
        "colab_type": "code",
        "outputId": "e539e401-a20d-47ef-f77f-bd46e106268c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "[w for w in tokenizer_porter('a runner likes running and runs a lot is very important')\n",
        "if w not in stop]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['runner', 'like', 'run', 'run', 'lot', 'veri', 'import']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4WHqhJNXSd2",
        "colab_type": "text"
      },
      "source": [
        "##Entrenando modelo usando REGRESIÓN LOGISTICA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br1OL8W2XW5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline                         # permite implementar métodos de ajuste y transformación\n",
        "from sklearn.linear_model import LogisticRegression           # modelo de regresión logistica\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer   #conversor de texto a vector\n",
        "from sklearn.model_selection import GridSearchCV              #búsqueda de cuadrícula con validación cruzada (para usar con regresión logistica)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VB1z3XtXwzG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# separamos los datos de entrenamiento y de pruebas\n",
        "X_train = df.loc[:25000, 'review'].values\n",
        "y_train = df.loc[:25000, 'sentiment'].values\n",
        "X_test = df.loc[25000:, 'review'].values\n",
        "y_test = df.loc[25000:, 'sentiment'].values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTsWnpW9YCmt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#propiedades de la conversor de texto a vectores\n",
        "tfidf = TfidfVectorizer(strip_accents=None,\n",
        "                        lowercase=False,\n",
        "                        preprocessor=None)\n",
        "def tokenizer(text):\n",
        "    return text.split()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDbMeTYeYh_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creación de dos diccionarios (1° para cálculos de TF-IDF, 2° para entrenar modelo con regresión logistica\n",
        "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
        "               'vect__stop_words': [stop, None],\n",
        "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
        "               'clf__penalty': ['l1', 'l2'],\n",
        "               'clf__C': [1.0, 10.0, 100.0]},\n",
        "              {'vect__ngram_range': [(1, 1)],\n",
        "               'vect__stop_words': [stop, None],\n",
        "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
        "               'vect__use_idf':[False],\n",
        "               'vect__norm':[None],\n",
        "               'clf__penalty': ['l1', 'l2'],\n",
        "               'clf__C': [1.0, 10.0, 100.0]}, ]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAfn0TRLY_6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#mezclamos regresión lineal y vectores de textos en un solo proceso\n",
        "lr_tfidf = Pipeline([('vect', tfidf),\n",
        "                     ('clf', LogisticRegression(random_state=0))])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75pQFTFvZUhC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
        "                           scoring='accuracy',\n",
        "                           cv=5,\n",
        "                           verbose=1,\n",
        "                           n_jobs=-1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uGVwXBsZgHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gs_lr_tfidf.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDXCWuClqs4S",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#**ANALISIS DE SENTIMIENTOS EN ESPAÑOL USANDO TWITTER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0Ztco6-b09l",
        "colab_type": "text"
      },
      "source": [
        "##Obtener Dataset de twitter para analizar\n",
        "https://github.com/jcsobrino/TFM-Analisis_sentimientos_Twitter-UOC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtzJHYMJcQz3",
        "colab_type": "text"
      },
      "source": [
        "###cargar librerias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esiw0R5hb9Ob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tweepy as tw\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E84yXpedZQ3",
        "colab_type": "text"
      },
      "source": [
        "###permisos de acceso desde python a el api rest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_hwgEk2culC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "consumer_key= 'nkjbpKyMEGEO8Ezy84figKZ0v'\n",
        "consumer_secret= 'eFdTEbucPvRNtXH74L98SjIg51Olc4LGcuTwL5hdhPBUfiu0Vq'\n",
        "access_token= '1176236145551826951-CyxNooeUEUscCt8j5Z5LOlDTtUzUJv'\n",
        "access_token_secret= 'mhsQ6rwRlXXtYGUhNgxfyh9P7PN6MwXxaGQEk4qOWJmAI'\n",
        "\n",
        "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tw.API(auth, wait_on_rate_limit=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Rc9RsrNNbxI",
        "colab_type": "text"
      },
      "source": [
        "###Subiendo un tweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roIJgdUoden3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "api.update_status('#USTATUNJA, subiendo mi primer tweet desde PYTHON 20200327 2.26pm')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ4mzeX-NfBu",
        "colab_type": "text"
      },
      "source": [
        "###Consultando tweets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzjhHn6vNrMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definir el termino de la busqueda y la fecha de inicio\n",
        "search_words = '#COVID19'\n",
        "date_since = '2020-03-27'\n",
        "#para que no tome los tweets que estar retweets\n",
        "new_search = search_words + \" -filter:retweets\"\n",
        "new_search\n",
        "\u000b# Collecional tweets\n",
        "tweets = tw.Cursor(api.search,\n",
        "              q=new_search,\n",
        "              lang=\"es\",\n",
        "              since=date_since).items(1000)\n",
        "tweets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnsyptWENvg3",
        "colab_type": "text"
      },
      "source": [
        "###Convertimos los tweets en una Dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reYRHI2zN0oG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_frame = [[tweet.user.screen_name, tweet.user.location,tweet.text] for tweet in tweets]\n",
        "\n",
        "tw_dataframe = pd.DataFrame(data= data_frame , columns=[\"user\",\"location\",\"text\"])\n",
        "tw_dataframe\n",
        "#guardamos el dataframe en un CSV\n",
        "tw_dataframe.to_csv('twitter_ClimateChange_data.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxiaP7qb2HIk",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktPgWvbqOEBm",
        "colab_type": "text"
      },
      "source": [
        "##P0. Obtener corpus : Convirtiendo XML a CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_N996vYOx2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#librerias necesarias\n",
        "import xml.etree.ElementTree as etree\n",
        "import csv\n",
        "from os import scandir\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhCauzL5jkpN",
        "colab_type": "text"
      },
      "source": [
        "###Funcion para listar archivos de un directorio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQfljH0ukvuM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing os module  \n",
        "import os \n",
        "#listado_de_archivos_desde_un_path\n",
        "def files_of_path(path): \n",
        "    return [obj.name for obj in os.scandir(path) if obj.is_file()]\n",
        "    \n",
        "files= files_of_path(\"/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/tass_2017/\")\n",
        "for file in files:\n",
        "    print(file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrG7haSNtb6T",
        "colab_type": "text"
      },
      "source": [
        "###función para convertir listas en archivos CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGxsb68nt1Co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def list_to_csv(data, filename):\n",
        "  with open(filename, 'w', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile, delimiter=',', lineterminator='\\n', quoting=csv.QUOTE_NONNUMERIC)\n",
        "    writer.writerows(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvec1PiF5Ipz",
        "colab_type": "text"
      },
      "source": [
        "###función para cargar de un CSV a  una LISTA (messages | labels)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBAnptZ04-We",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def csv_to_lists(filename):\n",
        "  messages = []\n",
        "  labels = []\n",
        "  with open(filename, 'r', encoding='utf-8') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',')\n",
        "    for row in reader:\n",
        "      messages.append(row[1])\n",
        "      labels.append(row[2])\n",
        "  return messages, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6wQAhv0V4RU",
        "colab_type": "text"
      },
      "source": [
        "###funciones para prasear xml en un dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm6bJ8rDnJ8d",
        "colab_type": "text"
      },
      "source": [
        "####corpus de general  tweetid | content | sentiments/polarity/value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSZHjRsUV9wj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def general_tass_to_list(filename):\n",
        "  tree = etree.parse(filename)\n",
        "  root = tree.getroot()\n",
        "  data = []\n",
        "\n",
        "  for tweet in root:\n",
        "    tweetId = tweet.find('tweetid').text\n",
        "    content = tweet.find('content').text\n",
        "    polarityValue = tweet.find('sentiments/polarity/value').text\n",
        "    data.append([tweetId, content.replace('\\n',' '), polarityValue])\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkQa78F3K6Vp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def general_tass_2017_to_list(filename,qrel=None):\n",
        "  tree = etree.parse(filename)\n",
        "  root = tree.getroot()\n",
        "  data = []\n",
        "\n",
        "  for tweet in root:\n",
        "    tweetId = tweet.find('tweetid').text\n",
        "    content = tweet.find('content').text\n",
        "    polarityValue = qrel[tweetId]\n",
        "    data.append([tweetId, content.replace('\\n',' '), polarityValue])\n",
        "\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYuKjPlHnvdV",
        "colab_type": "text"
      },
      "source": [
        "####Corpus politics  tweetid | content | sentiments/polarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iybrjrHmnvzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def politics_tass_to_list(filename):\n",
        "  tree = etree.parse(filename)\n",
        "  root = tree.getroot()\n",
        "  data = []\n",
        "\n",
        "  for tweet in root:\n",
        "    tweetId = tweet.find('tweetid').text\n",
        "    content = tweet.find('content').text\n",
        "    aux = next((e for e in tweet.findall('sentiments/polarity') if e.find('entity') == None), None)\n",
        "    if aux != None:\n",
        "      polarityValue = aux.find('value').text\n",
        "      data.append([tweetId, content.replace('\\n',' '), polarityValue])\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6a_3I3ZoTc6",
        "colab_type": "text"
      },
      "source": [
        "####corpus de internacional  tweetid | content | sentiments/polarity/value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGKq5CTgoT2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def intertass_tass_to_list(filename, qrel=None):\n",
        "  tree = etree.parse(filename)\n",
        "  root = tree.getroot()\n",
        "  data = []\n",
        "\n",
        "  for tweet in root:\n",
        "    tweetId = tweet.find('tweetid').text\n",
        "    content = tweet.find('content').text\n",
        "    polarityValue = tweet.find('sentiment/polarity/value').text\n",
        "    if polarityValue == None:\n",
        "      polarityValue = qrel[tweetId]\n",
        "      data.append([tweetId, content.replace('\\n',' '), polarityValue])\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0t1IZY6EzcB",
        "colab_type": "text"
      },
      "source": [
        "####Parcear corpus de social-TV (no esta terminado)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXVrTziIUB8x",
        "colab_type": "code",
        "outputId": "b9e1f823-286e-407d-f1f9-a1604ad1d1eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from lxml import etree\n",
        "doc = etree.parse('/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/tass_2017/Social-TV/socialtv-tweets-test.xml')\n",
        "#print (etree.tostring(doc,pretty_print=True ,xml_declaration=True, encoding=\"utf-8\"))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tweets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHREvJ4MUqQp",
        "colab_type": "code",
        "outputId": "760377aa-f7b1-4bd4-ddc9-c60288672949",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "raiz=doc.getroot()\n",
        "#print (raiz.tag)\n",
        "#print (len(raiz))\n",
        "tweet=raiz[1]\n",
        "print (tweet.text)\n",
        "print (tweet.attrib)\n",
        "print (tweet[0].text)\n",
        "for attr,value in tweet.items():\n",
        "  print (attr,value)\n",
        "print (tweet.get(\"sentiment\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El \n",
            "{'id': '456544890499760129'}\n",
            "Barça\n",
            "id 456544890499760129\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6cEJOPYP92J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from xml.dom import minidom\n",
        "def social_tv_to_list(filename):\n",
        "  xmldoc = minidom.parse(filename)\n",
        "  tweetlist = xmldoc.getElementsByTagName('tweet')\n",
        "  print(len(tweetlist))\n",
        "  print(tweetlist[0].attributes['id'].value)\n",
        "  sentimentlist = xmldoc.getElementsByTagName('sentiment')\n",
        "  print(sentimentlist[0].attributes['aspect'].value)\n",
        "  #for s in tweetlist:\n",
        "#    print(s.attributes['id'].value)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulBbULqvQg10",
        "colab_type": "code",
        "outputId": "1b8ddb7a-bdd6-4901-90ca-b62fb6bfc729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "social_tv_to_list(\"/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/tass_2017/Social-TV/socialtv-tweets-test.xml\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "456544890097131521\n",
            "Entrenador\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqoOWZwOsD6w",
        "colab_type": "text"
      },
      "source": [
        "####funcion para unir los tweets corpus general test con sus sentimientos\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YY91YATsEOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Listar los id tweets | sentiment :P (Positivo) - N (Negativo) - NEU (NEUtro) - NONE (sin sentimiento)\n",
        "def gold_standard_to_dict(filename):\n",
        "  with open(filename, 'r') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter='\\t')\n",
        "    data = {rows[0]: rows[1] for rows in reader}\n",
        "\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W712Ry66vFjl",
        "colab_type": "text"
      },
      "source": [
        "###Función para separar el 100% del corpus entre: Train : 70% - Test: 30%\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcDxtXKbvN4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_train_test_subsets(data, size):\n",
        "  codes = [d[0] for d in data]\n",
        "  labels = [d[2] for d in data]\n",
        "  codes_train, codes_test, labels_train, labels_test = train_test_split(codes, labels, train_size=size)\n",
        "  train_data = [d for d in data if d[0] in codes_train]\n",
        "  test_data = [d for d in data if d[0] in codes_test]\n",
        "  return train_data, test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwBHQQFsqR7i",
        "colab_type": "text"
      },
      "source": [
        "### **Ejecutar cada función de parsear los corpus y guardarlo en un CSV (full, train, test)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FmpPZyXqcyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = []\n",
        "\n",
        "#Parceamos el internacional TASS\n",
        "#tomamos el corpus internacional (test) y generamos una lista del ID del tweet y el sentimiento para agregarlo a la data\n",
        "qrel = gold_standard_to_dict(\"/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/tass/intertass-sentiment.qrel\")\n",
        "#como el test del corpus internacional esta sin los sentimientos es necesario agregarlos : qrel\n",
        "data.extend(intertass_tass_to_list(\"/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/tass/intertass-test.xml\", qrel))\n",
        "data.extend(intertass_tass_to_list(\"/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/tass_2017/InterTASS/InterTASS_development.xml\"))\n",
        "data.extend(intertass_tass_to_list(\"/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/tass_2017/InterTASS/InterTASS_Training.xml\"))\n",
        "#Parceamos el General\n",
        "#data.extend(DatasetHelper.general_tass_to_list(\"../datasets/tass_2017/InterTASS/general-test-tagged-3l.xml\"))\n",
        "#data.extend(DatasetHelper.general_tass_to_list(\"../datasets/tass_2017/InterTASS/general-train-tagged-3l.xml\"))\n",
        "qrel = gold_standard_to_dict(\"/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/tass_2017/General Corpus of TASS/general-sentiment-3l.qrel\")\n",
        "data.extend(general_tass_2017_to_list(\"/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/tass_2017/General Corpus of TASS/general-tweets-test.xml\", qrel))\n",
        "#Parceamos el STOMPOL (politica)\n",
        "data.extend(politics_tass_to_list(\"/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/tass/politics-test-tagged.xml\"))\n",
        "#separamos la data en train = 70%  | test = 30#\n",
        "test, train  = generate_train_test_subsets(data, size=0.3)\n",
        "list_to_csv(data,\"/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/dataset_2017_full.csv\")\n",
        "list_to_csv(train, '/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/dataset_2017_train.csv')\n",
        "list_to_csv(test, '/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/dataset_2017_test.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRF-yQ5j4qR6",
        "colab_type": "text"
      },
      "source": [
        "##P1. Preprocesamiento del corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlERt98W5qLZ",
        "colab_type": "text"
      },
      "source": [
        "###Cargar librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ0W5EYT5s2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re                                #operaciones regulares para la búsqueda y manipulación de cadenas\n",
        "from nltk import TweetTokenizer          #libreria para tokenizar\n",
        "from nltk.stem import SnowballStemmer    #algoritmo para clasificación de palabras\n",
        "#variables para mejorar la escritura (opcional)\n",
        "NORMALIZE = 'normalize'\n",
        "REMOVE = 'remove'\n",
        "MENTION = 'twmention'\n",
        "HASHTAG = 'twhashtag'\n",
        "URL = 'twurl'\n",
        "LAUGH = 'twlaugh'\n",
        "\n",
        "#definir que el algoritmo de clasificación use el idioma español\n",
        "_stemmer = SnowballStemmer('spanish')\n",
        "\n",
        "#definir una variable para la funcion de tokenizar (opcional)\n",
        "_tokenizer = TweetTokenizer().tokenize\n",
        "\n",
        "#variable para definir si quiero normalizar: normalize o eliminar: remove los hashtags, menciones y urls en los tweets\n",
        "_twitter_features=\"normalize\"\n",
        "#variable para definir si se desea tener convertir o no a la raiz de la palabra.\n",
        "_stemming=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrJ7vHEWNAbd",
        "colab_type": "text"
      },
      "source": [
        "### funciones/métodos de preprocesamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTB4zzjoL1Rs",
        "colab_type": "text"
      },
      "source": [
        "####listas de conversión (quitar tildes y palabras coloquiales) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Tj86lDGL6cU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#lista de conversión para quitar las tildes a las vocales.\n",
        "DIACRITICAL_VOWELS = [('á','a'), ('é','e'), ('í','i'), ('ó','o'), ('ú','u'), ('ü','u')]\n",
        "\n",
        "#lista para corregir algunas palabras coloquiales / jerga en español (obviamente faltan más)\n",
        "SLANG = [('d','de'), ('[qk]','que'), ('xo','pero'), ('xa', 'para'), ('[xp]q','porque'),('es[qk]', 'es que'),\n",
        "         ('fvr','favor'),('(xfa|xf|pf|plis|pls|porfa)', 'por favor'), ('dnd','donde'), ('tb', 'también'),\n",
        "         ('(tq|tk)', 'te quiero'), ('(tqm|tkm)', 'te quiero mucho'), ('x','por'), ('\\+','mas')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCCRddegNIDA",
        "colab_type": "text"
      },
      "source": [
        "#### funcion/método de normalización de risas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lvGhh5vNMEc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#metodo para normalizar las risas\n",
        "def normalize_laughs(message):\n",
        "  message = re.sub(r'\\b(?=\\w*[j])[aeiouj]{4,}\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'\\b(?=\\w*[k])[aeiouk]{4,}\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'\\b(juas+|lol)\\b', LAUGH, message, flags=re.IGNORECASE)\n",
        "  return message"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOW2UQjqN6Eh",
        "colab_type": "code",
        "outputId": "1eddf689-fca9-4954-d35d-1e2d596ba419",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print (normalize_laughs(\"esto muyy feliz jajajajaja o no tan feliz jejejejeje o mejor me rio a como papa noel JOJOJO o como en mileniams LOL  kakaka\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "esto muyy feliz twlaugh o no tan feliz twlaugh o mejor me rio a como papa noel twlaugh o como en mileniams twlaugh  twlaugh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3qBGgvpRgZL",
        "colab_type": "text"
      },
      "source": [
        "####Función/método para eliminar o normalizar  menciones, hashtags y URL de un mensaje (tweet)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpdPu3K8RukN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_twitter_features(message, twitter_features):\n",
        "\n",
        "  message = re.sub(r'[\\.\\,]http','. http', message, flags=re.IGNORECASE)\n",
        "  message = re.sub(r'[\\.\\,]#', '. #', message)\n",
        "  message = re.sub(r'[\\.\\,]@', '. @', message)\n",
        "\n",
        "  if twitter_features == REMOVE:\n",
        "    # eliminar menciones, hashtags y URL\n",
        "    message = re.sub(r'((?<=\\s)|(?<=\\A))(@|#)\\S+', '', message)\n",
        "    message = re.sub(r'\\b(https?:\\S+)\\b', '', message, flags=re.IGNORECASE)\n",
        "  elif twitter_features == NORMALIZE:\n",
        "    # cuando sea necesario se normalizaran las menciones, hashtags y URL\n",
        "    message = re.sub(r'((?<=\\s)|(?<=\\A))@\\S+', MENTION, message)\n",
        "    message = re.sub(r'((?<=\\s)|(?<=\\A))#\\S+', HASHTAG, message)\n",
        "    message = re.sub(r'\\b(https?:\\S+)\\b', URL, message, flags=re.IGNORECASE)\n",
        "\n",
        "  return message"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS3BzP5OR_hJ",
        "colab_type": "code",
        "outputId": "7f23845d-e0bb-4dcd-da87-5eda8ae68e26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(process_twitter_features(\"Rosell, una noche. Adivina quien!! http://t.co/PPAwijRX\",\"normalize\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rosell, una noche. Adivina quien!! twurl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCS2HmN4V_hi",
        "colab_type": "text"
      },
      "source": [
        "####Función/método general para el preprocesamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3T1DgbnWFYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(message):\n",
        "  # convertir a minusculas\n",
        "  message = message.lower()\n",
        "        \n",
        "  # eliminar números, retorno de linea y el tan odios retweet (de los viejos estilos de twitter)\n",
        "  message = re.sub(r'(\\d+|\\n|\\brt\\b)', '', message)\n",
        "        \n",
        "  # elimar vocales con signos diacríticos (posible ambigüedad)\n",
        "  for s,t in DIACRITICAL_VOWELS:\n",
        "    message = re.sub(r'{0}'.format(s), t, message)\n",
        "        \n",
        "  # eliminar caracteres repetidos \n",
        "  message = re.sub(r'(.)\\1{2,}', r'\\1\\1', message)\n",
        "       \n",
        "  # normalizar las risas\n",
        "  message = normalize_laughs(message)\n",
        "        \n",
        "  # traducir la jerga y terminos coloquiales sobre todo en el español\n",
        "  for s,t in SLANG:\n",
        "    message = re.sub(r'\\b{0}\\b'.format(s), t, message)\n",
        "\n",
        "  #normalizar/eliminar hashtags, menciones y URL\n",
        "  message = process_twitter_features(message, _twitter_features)\n",
        "\n",
        "  #Convertir las palabras a su raiz ( Bonita, bonito) -> bonit \n",
        "  if _stemming:\n",
        "    message = ' '.join(_stemmer.stem(w) for w in _tokenizer(message))\n",
        "\n",
        "  return message"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCkSi7RJW6FX",
        "colab_type": "code",
        "outputId": "5819667f-a98d-490c-9364-84eb72492cf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(preprocess(\"LOL!! muy graciosa esta paguina https://actualidadpanamericana.com :-) jajajaja muy buena\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "twlaugh!! muy graciosa esta paguina twurl :-) twlaugh muy buena\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aWOdOb167P7",
        "colab_type": "text"
      },
      "source": [
        "###Descargamos la librerias  NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSp_L2h2_I36",
        "colab_type": "code",
        "outputId": "1399a07a-8ab9-4f45-ed20-dc69ab9b9678",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Descargamos la libreria de stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIJQSCntSPOl",
        "colab_type": "text"
      },
      "source": [
        "### Cargamos el CSV del corpus de google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVtJMBDy_slb",
        "colab_type": "code",
        "outputId": "fb40ed85-28f2-4121-be6d-f6dddf4997da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOE6fCqXcUXn",
        "colab_type": "text"
      },
      "source": [
        "###Aplicamos preprocesamiento al CSV y creamos un nuevo CSV limpio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ods0bwtGCWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/dataset_2017_full.csv', encoding='utf-8')\n",
        "#asignamos nombres a las columnas del csv para facilitar la busqueda de información\n",
        "df.columns = ['tweetid', 'tweet','sentiment']\n",
        "#aplicamos el preprocesamiento a los tweets con steaming =false\n",
        "df['tweet'] = df['tweet'].apply(preprocess)\n",
        "#eliminamos la columna tweetid que no nos sirve para entrenar y si nos genera mas uso de memoria \n",
        "df = df.drop(columns=\"tweetid\")\n",
        "#Es mejor trabajar con valores enteros que con letras\n",
        "#por lo tanto reemplazaremos los sentimientos que estan como NONE->-1 | NEU -> 0 | P->1 | N->2\n",
        "df.loc[df['sentiment'] == 'NONE', 'sentiment'] = '-1'\n",
        "df.loc[df['sentiment'] == 'NEU', 'sentiment'] = '0'\n",
        "df.loc[df['sentiment'] == 'P', 'sentiment'] = '1'\n",
        "df.loc[df['sentiment'] == 'N', 'sentiment'] = '2'\n",
        "df[\"sentiment\"].unique()\n",
        "#guardamos el dataset en un nuvevo CSV para facilitar su posterior uso\n",
        "df.to_csv('/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/dataset_2017_full_clean.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAn4p8Z_Y8sH",
        "colab_type": "text"
      },
      "source": [
        "##P2.Entrenando el modelo de aprendizaje"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3kl_mdMN4ec",
        "colab_type": "text"
      },
      "source": [
        "###Funciones de tokenizar/extraer tweets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5UTVDCZZBYs",
        "colab_type": "code",
        "outputId": "dc5c3650-e65b-45de-9ac4-1782061396bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#p2.1: funcion tokenizar con esteroides --tokeniza y limpia--\n",
        "print(\"p2.1: funcion tokenizar con esteroides --tokeniza y limpia--\")\n",
        "def tokenizer(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
        "    text = re.sub('[\\W]+', ' ', text.lower()) +' '.join(emoticons).replace('-', '')\n",
        "    tokenized = [w for w in text.split() if w not in stop]\n",
        "    return tokenized\n",
        "#p2.2: funcion para extraer un documento del dataset  \n",
        "print(\"p2.2: funcion para extraer un documento del dataset  \")\n",
        "def stream_docs(path):\n",
        "    with open(path, 'r', encoding='utf-8') as csv:\n",
        "        next(csv)  # skip header\n",
        "        for line in csv:\n",
        "            text, label = line[:-3],  int(line[-2])\n",
        "            yield text, label\n",
        "#p2.3: funcion que tomara una secuencia de documentos y devolvera un número particular de documentos\n",
        "def get_minibatch(doc_stream, size):\n",
        "    docs, y = [], []\n",
        "    try:\n",
        "        for _ in range(size):\n",
        "            text, label = next(doc_stream)\n",
        "            docs.append(text)\n",
        "            y.append(label)\n",
        "    except StopIteration:\n",
        "        return None, None\n",
        "    return docs, y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "p1.1: funcion tokenizar con esteroides --tokeniza y limpia--\n",
            "p1.2: funcion para extraer un documento del dataset  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuILfqd2hq1O",
        "colab_type": "code",
        "outputId": "954ba14a-b9e1-435d-fbdf-cb823088aacb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "next(stream_docs(path='/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/dataset_2017_full_clean.csv'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('twmention ya era hora de volver al csgo y dejares el padel bienvenida ', 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fcBk57OXCz3",
        "colab_type": "text"
      },
      "source": [
        "###Entrenamos el modelo usando regresión logistica\n",
        "usaremos regresión logistica xq es menos costoso en tiempo de procesamiento que support vector machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCwTY8TCM3HE",
        "colab_type": "code",
        "outputId": "f423b014-52cb-4b0f-8989-a6e80495e9e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "path='/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/dataset_2017_full_clean.csv'\n",
        "#p2: definimos una versión liviana de CountVectorizer+TfidfVectorizer llamada HashingVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "vect = HashingVectorizer(decode_error='ignore', \n",
        "                         n_features=2**21,\n",
        "                         preprocessor=None, \n",
        "                         tokenizer=tokenizer)\n",
        "\n",
        "#definimos como algoritmo la regressión logistica en el decenso gradiante \n",
        "\n",
        "clf = SGDClassifier(loss='log', random_state=1, max_iter=1)\n",
        "doc_stream = stream_docs(path)\n",
        "#p3. entrenamos \n",
        "import re\n",
        "import numpy as np\n",
        "import pyprind\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('spanish')\n",
        "pbar = pyprind.ProgBar(50)\n",
        "#definimos las clases con las cuales vamos a entrenar\n",
        "classes = np.array([-1,0, 1,2])\n",
        "#hacemos 50 repeticiones\n",
        "for _ in range(50):\n",
        "  #tomaremos grupos de 500 tweets para entrenar\n",
        "    X_train, y_train = get_minibatch(doc_stream, size=500)\n",
        "    if not X_train:\n",
        "        break\n",
        "    X_train = vect.transform(X_train)\n",
        "    clf.partial_fit(X_train, y_train, classes=classes)\n",
        "    pbar.update()\n",
        "#probamos la eficiencia del modelo con 500 tweets .\n",
        "X_test, y_test = get_minibatch(doc_stream, size=500)\n",
        "X_test = vect.transform(X_test)\n",
        "print('Presición del modelo: %.3f' % clf.score(X_test, y_test))\n",
        "#recalibramos el modelo.\n",
        "clf = clf.partial_fit(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0% [##############################] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:06\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Presición del modelo: 0.844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLOtd2p7X0yn",
        "colab_type": "text"
      },
      "source": [
        "##P3.Serializamos (congelamos) el modelo para usarlo fuera de google colaboratory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRxlktL3X-Q_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "#creo una carpeta en mi google drive para guardar los archivos serializados\n",
        "dest = os.path.join('/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/twitterclassifier', 'pkl_objects')\n",
        "if not os.path.exists(dest):\n",
        "    os.makedirs(dest)\n",
        "#convertimos el clasificador y el stopword en archivo/objectos pkl\n",
        "pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=4)   \n",
        "pickle.dump(clf, open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol=4)\n",
        "#Es importante recordar que deben verificar que los dos archivos esten en su drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atFbFUcmZuwZ",
        "colab_type": "text"
      },
      "source": [
        "###Probemos a ver si funciona\n",
        "Cambiamos la basepath (directorio por defecto) de Python a la carpeta de **Twitterclassifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moUaKeQOZye5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/twitterclassifier')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWSBw2L6aJri",
        "colab_type": "text"
      },
      "source": [
        "####Deserializamos los estimadores "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCQ2qYrsZ_67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import re\n",
        "import os\n",
        "from vectorizer import vect\n",
        "clf = pickle.load(open(os.path.join('pkl_objects', 'classifier.pkl'), 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR3R_vU3aY_L",
        "colab_type": "text"
      },
      "source": [
        "#### Clasifiquemos un texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVpOVv8tacSf",
        "colab_type": "code",
        "outputId": "05b7b295-6bba-4016-93f2-24736e4278c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import numpy as np\n",
        "#NONE->-1 | NEU -> 0 | P->1 | N->2\n",
        "label = {-1:'Sin sentimiento', 0:'Neutro', 1:'Positivo',2: 'Negativo'}\n",
        "\n",
        "#example = ['Te odio más que a la muerte']\n",
        "example1 = 'covid19 te ODIOOOO'\n",
        "example = [example1]\n",
        "#convertimos el texto en un vector de palabras y extraemos sus caracteristicas https://scikit-learn.org/stable/modules/feature_extraction.html\n",
        "textConvert = vect.transform(example)  \n",
        "print('*Predicción: %s\\n*Probabilidad: %.2f%%'%(label[clf.predict(textConvert)[0]], np.max(clf.predict_proba(textConvert))*100))\n",
        "print('*Predicción: %s'%label[clf.predict(textConvert)[0]])\n",
        "print(np.max(clf.predict_proba(textConvert))*100)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*Predicción: Positivo\n",
            "*Probabilidad: 63.94%\n",
            "*Predicción: Positivo\n",
            "63.944211933486194\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiZauNoCbrxG",
        "colab_type": "text"
      },
      "source": [
        "###**RECORREMOS LOS TWEETS DESCARGADOS Y LOS CLASIFICAMOS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL1EUqECSjP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyprind\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/TWEETS_LGGG_COVID19.csv', encoding='utf-8')\n",
        "#creamos una columna llamada Sentimient donde guardaremos la predicción\n",
        "df['sentiment'] =''\n",
        "#creamos una columna llamada Probability donde guardaremos la acertabilidad que dio el clasificador\n",
        "df['probability']=0\n",
        "#conversión de sentimientos (numeros a palabras)= NONE->-1 | NEU -> 0 | P->1 | N->2\n",
        "label = {-1:'Sin sentimiento', 0:'Neutro', 1:'Positivo',2: 'Negativo'}\n",
        "for rowid in range(len(df.index)):\n",
        "  text=df['text'][rowid]\n",
        "  textConvert = vect.transform([text]) \n",
        "  df['sentiment'][rowid]=label[clf.predict(textConvert)[0]]\n",
        "  df['probability'][rowid]=np.max(clf.predict_proba(textConvert))*100\n",
        "  pbar.update()\n",
        "df.head(20)\n",
        "#df.to_csv('/content/drive/My Drive/IA/Analisis_sentimientos_Twitter/datasets/TWEETS_LGGG_COVID19_analysis.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Noxdqzav-LF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#segunda forma de ejecutar el analisis (metodos)\n",
        "def f_prediction(row):\n",
        "  text=row['text']\n",
        "  textConvert = vect.transform([text]) \n",
        "  return label[clf.predict(textConvert)[0]]\n",
        "\n",
        "def f_probability(row):\n",
        "  text=row['text']\n",
        "  textConvert = vect.transform([text]) \n",
        "  return np.max(clf.predict_proba(textConvert))*100\n",
        "\n",
        "df[\"sentiment\"] = df.apply(f_prediction, axis=1) # recorriendo columnas\n",
        "df[\"probability\"] = df.apply(f_probability, axis=1) # recorriendo columnas\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Lh2gY17uFeG",
        "colab_type": "code",
        "outputId": "8823138c-c146-4b0b-e9bf-d7956a596d33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#sentimientos = df[\"sentiment\"].unique()\n",
        "df.groupby('sentiment')['location'].nunique().plot(kind='bar')\n",
        "print(df.groupby(['sentiment']).size())\n",
        "#df.groupby(['sentiment']).size().unstack().plot(kind='bar',stacked=True)\n",
        "plt.show()\n",
        "\n",
        "#df.head(20)\n",
        "#df[\"sentiment\"].unique()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentiment\n",
            "Negativo      2\n",
            "Positivo    682\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}